% Chapter 7: Conclusion

\chapter{Conclusion}
\label{ch:conclusion}

This thesis has developed a unified framework for sequential decision-making in non-stationary, heavy-tailed environments. We synthesize our contributions and outline directions for future research.

\section{Summary of Contributions}

\subsection{Theoretical Contributions}

\subsubsection{1. The Information-Theoretic Foundation}

We derived the Kullback-Leibler divergence for the augmented observation space $y_t = [r_t, v_t]$, proving that:
\begin{equation}
\KL^{\text{total}} = \KL^{\text{Gaussian}} + \KL^{\text{Gamma}} \approx 0.6 + 2.7 = 3.3 \text{ nats}
\end{equation}

This 5$\times$ information gain explains why volatility augmentation dramatically accelerates regime detection --- each observation provides five times more discriminative power between Bull and Bear regimes.

\subsubsection{2. The Martingale Structure}

We proved that Bayesian posterior beliefs form a bounded martingale with respect to the observation filtration (Theorem~\ref{thm:posterior_martingale}). This result establishes that the agent cannot ``predict'' its own future belief changes --- improvements in regime inference arise solely from genuine information in the observations.

\subsubsection{3. The Impossibility Theorem}

The central theoretical contribution (Theorem~\ref{thm:impossibility}) proves that in infinite-variance environments:
\begin{quote}
\emph{Simultaneous maximization of logarithmic growth rate and boundedness of maximum drawdown is impossible.}
\end{quote}

Any strategy achieving bounded drawdowns must sacrifice a positive fraction of the maximum growth rate. The ``cost of survival'' is not a failure of optimization but a mathematical necessity.

\subsection{Methodological Contributions}

\subsubsection{1. The Volatility-Augmented HMM}

We developed a novel HMM that:
\begin{itemize}
    \item Uses bivariate observations $[r_t, v_t]$ with Gaussian$\times$Gamma likelihood
    \item Integrates CUSUM change-point detection for accelerated response
    \item Operates in log-space for numerical stability
    \item Achieves $\approx 1.9$-step detection lag (vs. $>20$ steps for return-only HMM)
\end{itemize}

\subsubsection{2. The Risk-Constrained Kelly Framework}

We integrated CPPI floor protection with Kelly betting:
\begin{equation}
f_t = \min\left(f^*, m \cdot \frac{W_t - F_t}{W_t}\right)
\end{equation}

This mechanism:
\begin{itemize}
    \item Enforces drawdown constraints dynamically
    \item Achieves 100\% survival rate in Student-$t$ environments with $\nu = 3$
    \item Maintains 0\% drawdown breach rate
\end{itemize}

\subsubsection{3. The Three-Layer Architecture}

The complete system integrates:
\begin{enumerate}
    \item \textbf{Detection Layer}: HMM infers regime from observations
    \item \textbf{Decision Layer}: Kelly computes optimal bet given regime probabilities
    \item \textbf{Protection Layer}: CPPI enforces floor as last resort
\end{enumerate}

\subsection{Empirical Contributions}

Monte Carlo simulations validated:
\begin{enumerate}
    \item Near-instantaneous regime detection ($<2$ steps)
    \item 100\% survival rate under extreme heavy tails
    \item Quantified cost of survival: $\approx$10 pp CAGR for 20\% drawdown protection
    \item The concave efficient frontier mapping growth versus safety
\end{enumerate}

\section{Relation to Prior Work}

This thesis bridges two historically separate literatures:

\begin{center}
\begin{tabular}{lcc}
\toprule
Aspect & Prior Work & This Thesis \\
\midrule
Learning & Bayesian Kelly \cite{maclean2011} & Regime-switching HMM \\
Risk management & Static constraints \cite{busseti2016} & Dynamic CPPI \\
Environment & Gaussian & Heavy-tailed Student-$t$ \\
Observation & Returns only & Returns + volatility \\
\bottomrule
\end{tabular}
\end{center}

The key innovation is the \emph{integration} of learning and risk management in a non-stationary, heavy-tailed environment --- a setting where prior approaches fail.

\section{Implications for Practice}

\subsection{For Portfolio Managers}

\begin{enumerate}
    \item \textbf{Monitor Volatility}: Volatility spikes are more informative than returns for detecting regime changes
    
    \item \textbf{Accept the Trade-off}: Bounded drawdowns require sacrificing growth --- attempting to achieve both leads to ruin
    
    \item \textbf{Deleverage Early}: Better to deleverage on a false alarm than to miss a true regime shift
\end{enumerate}

\subsection{For Risk Managers}

\begin{enumerate}
    \item \textbf{Reject Gaussian Assumptions}: Standard VaR/CVaR calculations are invalid under heavy tails
    
    \item \textbf{Use Dynamic Floors}: Static constraint optimization fails; dynamic CPPI adapts to market conditions
    
    \item \textbf{Quantify Gap Risk}: In discrete time, floor breaches are possible --- size them using Student-$t$ tail probabilities
\end{enumerate}

\section{Limitations}

\subsection{Modeling Assumptions}

\begin{enumerate}
    \item \textbf{Known Parameters}: We assume regime parameters $(\mu, \sigma, k, \theta)$ are known. In practice, these must be estimated, adding uncertainty.
    
    \item \textbf{Two Regimes}: Real markets may have multiple regimes or continuous regime spectra.
    
    \item \textbf{No Transaction Costs}: Frequent rebalancing during regime transitions incurs costs not modeled here.
\end{enumerate}

\subsection{Computational Limitations}

\begin{enumerate}
    \item \textbf{Fixed Window Length}: The volatility window $L$ is fixed; adaptive selection could improve performance.
    
    \item \textbf{Simplified CUSUM}: More sophisticated change-point detection (e.g., PELT, BOCPD) could reduce false positives.
\end{enumerate}

\section{Future Work}

\subsection{Multi-Asset Extension}

Extend the framework to portfolios with correlated assets:
\begin{equation}
y_t = \begin{pmatrix} r_t^{(1)} \\ \vdots \\ r_t^{(n)} \\ \text{vec}(\Sigma_t) \end{pmatrix}
\end{equation}
where $\Sigma_t$ is the realized covariance matrix, modeled as Wishart-distributed.

\subsection{Continuous-Time Formulation}

Embed the model in a continuous-time framework using jump-diffusions:
\begin{equation}
dS_t = \mu_{Z_t} S_t \, dt + \sigma_{Z_t} S_t \, dW_t + S_{t^-} \, dJ_t
\end{equation}
where $Z_t$ is a continuous-time Markov chain and $J_t$ is a compound Poisson jump process.

This formulation would enable:
\begin{itemize}
    \item Closed-form bounds on detection lag
    \item Rigorous proofs of CPPI guarantees
    \item Connection to stochastic control theory
\end{itemize}

\subsection{Bayesian Parameter Learning}

Replace fixed parameters with fully Bayesian treatment:
\begin{equation}
p(\theta \mid y_{1:T}) \propto p(y_{1:T} \mid \theta) \cdot p(\theta)
\end{equation}
using Hamiltonian Monte Carlo for posterior sampling.

\subsection{Reinforcement Learning Comparison}

Compare the Bayesian agent to model-free approaches:
\begin{itemize}
    \item Deep Q-Networks (DQN)
    \item Proximal Policy Optimization (PPO)
    \item Can ``model-free'' approaches rediscover the Kelly constraints?
\end{itemize}

\section{Concluding Remarks}

The Kelly criterion, discovered in 1956, promised an elegant solution to the problem of optimal betting: maximize the expected logarithm of wealth, and you will grow rich faster than anyone else. Nearly seven decades later, this promise remains valid --- but incomplete.

Real financial markets are neither stationary nor thin-tailed. The Gaussian universe in which Kelly's formula was derived does not exist. In the heavy-tailed, regime-switching reality of actual markets, the naive application of Kelly betting leads to catastrophic ruin.

This thesis has shown that \textbf{survival in heavy-tailed markets requires explicit uncertainty quantification and risk constraints}. The synthesis of:
\begin{enumerate}
    \item Bayesian inference (via Volatility-Augmented HMM)
    \item Stochastic process theory (via martingale convergence)
    \item Convex optimization (via CPPI constraints)
\end{enumerate}
provides a theoretically rigorous and practically implementable framework for sequential decision-making under genuine uncertainty.

The Impossibility Theorem formalizes what practitioners have long suspected: there is no free lunch. Survival costs growth, and growth costs survival. The efficient frontier of survival quantifies this trade-off precisely, enabling informed decisions about how much growth to sacrifice for how much protection.

In the words of Edward Thorp, the greatest Kelly practitioner: ``Don't go broke.'' This thesis has shown \emph{how} to not go broke --- and exactly \emph{what it costs}.
