\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

\title{Formal Model Definition: Bayesian Sequential Decision-Making}
\author{Honors Thesis}
\date{\today}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{problem}{Problem}

\begin{document}

\maketitle

\section{The Probability Space}

\begin{definition}[Filtered Probability Space]
Let $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t \in \mathbb{N}_0}, \mathbb{P})$ be a filtered probability space, where:
\begin{itemize}
    \item $\Omega$ is the sample space of all possible outcomes
    \item $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$
    \item $\{\mathcal{F}_t\}_{t=0}^{\infty}$ is a filtration satisfying $\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}$
    \item $\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$
\end{itemize}
\end{definition}

The filtration $\mathcal{F}_t$ represents the \textit{information available to the agent at time $t$}. Following standard discrete-time finance conventions (Shreve), we define:
\[
\mathcal{F}_t = \sigma(r_1, r_2, \ldots, r_t),
\]
the $\sigma$-algebra generated by returns observed up to and including time $t$. We set $\mathcal{F}_0 = \{\emptyset, \Omega\}$ (trivial $\sigma$-algebra).

\begin{assumption}[Predictability Constraint]
The agent's betting fraction $f_t$ represents the bet placed at time $t-1$ for the period $[t-1, t]$. It must be $\mathcal{F}_{t-1}$-measurable:
\[
f_t \in \mathcal{F}_{t-1} \quad \text{for all } t \geq 1.
\]
This ensures that the decision is based only on information available \textit{before} the return $r_t$ is realized, preventing \textit{look-ahead bias}.
\end{assumption}

\begin{assumption}[Adapted Returns]
The return process $\{r_t\}_{t=1}^{\infty}$ is \textit{adapted} to the filtration, meaning
\[
r_t \in \mathcal{F}_{t} \quad \text{for all } t \geq 1.
\]
That is, the return $r_t$ becomes known at time $t$ (after the fraction $f_t$ was chosen at time $t-1$).
\end{assumption}

\textbf{Timing Convention}: At time $t$, the agent:
\begin{enumerate}
    \item Observes wealth $W_t$ and all returns $\{r_1, \ldots, r_t\}$
    \item Decides the betting fraction $f_{t+1}$ for the next period $[t, t+1]$
\end{enumerate}

\section{The Wealth Dynamics}

\begin{definition}[Discrete-Time Wealth Update]
Let $W_t$ denote the agent's wealth at time $t$. The wealth evolves according to
\[
W_t = W_{t-1}(1 + f_t r_t), \quad t \geq 1,
\]
with initial wealth $W_0 > 0$.
\end{definition}

For the Kelly Criterion, we analyze the \textit{log-wealth process}:
\[
\log W_t = \log W_{t-1} + \log(1 + f_t r_t).
\]

Using the Taylor expansion $\log(1 + x) = x - \frac{x^2}{2} + O(x^3)$ for small $|x|$:
\[
\log W_t \approx \log W_{t-1} + f_t r_t - \frac{(f_t r_t)^2}{2}.
\]

Summing over $t = 1, 2, \ldots, T$:
\[
\log W_T \approx \log W_0 + \sum_{t=1}^{T} \left( f_t r_t - \frac{f_t^2 r_t^2}{2} \right).
\]

Taking expectations (assuming $\mathbb{E}[r_t \mid \mathcal{F}_t] = \mu_t$ and $\text{Var}(r_t \mid \mathcal{F}_t) = \sigma_t^2$):
\[
\mathbb{E}[\log W_T \mid \mathcal{F}_0] \approx \log W_0 + \sum_{t=1}^{T} \left( f_t \mu_t - \frac{f_t^2 (\sigma_t^2 + \mu_t^2)}{2} \right).
\]

\section{The Environment}

\subsection{Static Regimes}

\subsubsection{Regime 1: Gaussian Log-Returns}

Assume returns $r_t$ are i.i.d. Gaussian:
\[
r_t \sim \mathcal{N}(\mu, \sigma^2), \quad t = 1, 2, \ldots
\]

\subsubsection{Regime 2: Heavy-Tailed Returns (Student-t)}

To model \textit{fat tails} (extreme events), we use a Student-t distribution:
\[
r_t \sim t_{\nu}(\mu, \sigma^2), \quad \nu = 3,
\]
where $\nu$ is the degrees of freedom. For $\nu = 3$, the distribution has infinite fourth moment, making variance estimation unreliable and inducing rare but severe drawdowns.

\subsection{Regime Switching (Hidden Markov Model)}

\begin{definition}[Hidden State Process]
Let $\{S_t\}_{t=1}^{\infty}$ be a discrete-time Markov chain on the state space $\mathcal{S} = \{1, 2, \ldots, M\}$, where $M$ is the number of regimes. The state $S_t$ is \textit{hidden} (not directly observable).
\end{definition}

\begin{definition}[Transition Matrix]
The regime dynamics are governed by a transition matrix $P \in \mathbb{R}^{M \times M}$, where
\[
P_{ij} = \mathbb{P}(S_{t+1} = j \mid S_t = i), \quad \sum_{j=1}^{M} P_{ij} = 1.
\]
\end{definition}

\begin{definition}[Regime-Dependent Returns]
Conditional on the hidden state $S_t = s$, the return $r_t$ is drawn from a regime-specific distribution with parameters $\theta_s$:
\[
r_t \mid S_t = s \sim p(r \mid \theta_s).
\]
For example, with $M = 2$ regimes:
\begin{align*}
\theta_1 &= (\mu_1, \sigma_1) \quad \text{(Gaussian: ``Bull Market'')}, \\
\theta_2 &= (\mu_2, \sigma_2, \nu) \quad \text{(Student-t: ``Bear Market'' with heavy tails)}.
\end{align*}
\end{definition}

\section{The Optimization Problem}

\subsection{Kelly Objective}

\begin{problem}[Kelly Criterion]
Maximize the long-run logarithmic growth rate:
\[
f^* = \arg\max_{f_t} \mathbb{E}\left[ \log(1 + f_t r_t) \mid \mathcal{F}_t \right].
\]
\end{problem}

For i.i.d. Gaussian returns $r_t \sim \mathcal{N}(\mu, \sigma^2)$, the optimal Kelly fraction is:
\[
f^* = \frac{\mu}{\sigma^2}.
\]

\subsection{Risk Constraint}

To prevent catastrophic losses in finite time, we impose two types of risk constraints:

\begin{definition}[Drawdown]
The drawdown at time $t$ is defined as
\[
D_t = \max_{0 \leq s \leq t} W_s - W_t.
\]
The relative drawdown is $D_t / \max_{0 \leq s \leq t} W_s$.
\end{definition}

\begin{problem}[Probability of Drawdown Constraint]
Limit the probability of exceeding a drawdown threshold $\alpha$:
\[
\mathbb{P}(D_t > \alpha) \leq \beta,
\]
where $\alpha$ is the maximum acceptable drawdown (e.g., 50\% of peak wealth) and $\beta$ is the risk tolerance (e.g., 5\%).
\end{problem}

\begin{definition}[Conditional Value at Risk (CVaR)]
For a random variable $X$ (e.g., portfolio return), the $\alpha$-CVaR is
\[
\text{CVaR}_{\alpha}(X) = \mathbb{E}[X \mid X \leq \text{VaR}_{\alpha}(X)],
\]
where $\text{VaR}_{\alpha}(X)$ is the $\alpha$-quantile of $X$.
\end{definition}

\begin{problem}[CVaR-Constrained Kelly Optimization]
For a portfolio of $K$ assets with returns $r_1, \ldots, r_K$, solve:
\begin{align*}
\max_{f_1, \ldots, f_K} \quad & \mathbb{E}\left[ \log\left(1 + \sum_{i=1}^{K} f_i r_i\right) \right] \\
\text{subject to} \quad & \text{CVaR}_{0.05}\left( \sum_{i=1}^{K} f_i r_i \right) \geq -\tau, \\
& \sum_{i=1}^{K} f_i \leq 1, \\
& f_i \geq 0, \quad i = 1, \ldots, K,
\end{align*}
where $\tau$ is the maximum acceptable loss in the worst 5\% of scenarios (e.g., $\tau = 0.20$ for 20\% max loss).
\end{problem}

\section{Connection to Multi-Armed Bandits}

In the multi-armed bandit (MAB) setting, the agent must simultaneously:
\begin{enumerate}
    \item \textbf{Explore}: Learn the unknown means $\mu_1, \ldots, \mu_K$ of $K$ arms.
    \item \textbf{Exploit}: Allocate wealth using Kelly fractions based on current estimates.
\end{enumerate}

Let $\hat{\mu}_{i,t}$ denote the Bayesian posterior mean of arm $i$ at time $t$. Then the Kelly allocation becomes:
\[
f_{i,t+1} = \frac{\hat{\mu}_{i,t}}{\hat{\sigma}_{i,t}^2}, \quad i = 1, \ldots, K,
\]
subject to the constraints above. The challenge is that incorrect estimates (due to insufficient exploration) lead to suboptimal growth.

\section{Bayesian Belief Propagation}

For the HMM regime-switching model, the agent maintains a \textit{belief state} over the hidden regime.

\begin{definition}[Belief State]
Let $\pi_t(i) = \mathbb{P}(S_t = i \mid \mathcal{F}_t)$ denote the posterior probability that the system is in regime $i$ at time $t$, given all observed returns.
\end{definition}

\begin{problem}[Bayesian Update (Bayes' Rule)]
The belief evolves recursively via:
\[
\pi_t(i) = \frac{\mathcal{L}(r_t \mid S_t = i) \sum_{j=1}^{M} P_{ji} \pi_{t-1}(j)}{\sum_{k=1}^{M} \mathcal{L}(r_t \mid S_t = k) \sum_{j=1}^{M} P_{jk} \pi_{t-1}(j)},
\]
where $\mathcal{L}(r_t \mid S_t = i) = p(r_t \mid \theta_i)$ is the likelihood of observing $r_t$ in regime $i$.
\end{problem}

\textbf{Interpretation}:
\begin{enumerate}
    \item \textbf{Prediction}: $\sum_{j=1}^{M} P_{ji} \pi_{t-1}(j)$ propagates yesterday's belief forward using the transition matrix.
    \item \textbf{Update}: Multiply by the likelihood $\mathcal{L}(r_t \mid S_t = i)$ and normalize.
\end{enumerate}

The agent uses $\pi_t$ to compute regime-weighted Kelly fractions:
\[
f_{t+1} = \sum_{i=1}^{M} \pi_t(i) \cdot f^*_i, \quad \text{where } f^*_i = \frac{\mu_i}{\sigma_i^2}.
\]

\end{document}
