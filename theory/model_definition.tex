\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

\title{Formal Model Definition: Bayesian Sequential Decision-Making}
\author{Honors Thesis}
\date{\today}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{problem}{Problem}

\begin{document}

\maketitle

\section{The Probability Space}

\begin{definition}[Filtered Probability Space]
Let $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t \in \mathbb{N}_0}, \mathbb{P})$ be a filtered probability space, where:
\begin{itemize}
    \item $\Omega$ is the sample space of all possible outcomes
    \item $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$
    \item $\{\mathcal{F}_t\}_{t=0}^{\infty}$ is a filtration satisfying $\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}$
    \item $\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$
\end{itemize}
\end{definition}

The filtration $\mathcal{F}_t$ represents the \textit{information available to the agent at time $t$}. Formally, $\mathcal{F}_t = \sigma(r_0, r_1, \ldots, r_{t-1})$ is the $\sigma$-algebra generated by the history of returns up to (but not including) time $t$.

\begin{assumption}[Predictability Constraint]
The agent's betting fraction $f_t$ must be $\mathcal{F}_t$-measurable, i.e.,
\[
f_t \in \mathcal{F}_t \quad \text{for all } t \in \mathbb{N}_0.
\]
This ensures that the decision at time $t$ is based only on past information, preventing \textit{look-ahead bias}.
\end{assumption}

\begin{assumption}[Adapted Returns]
The return process $\{r_t\}_{t=0}^{\infty}$ is \textit{adapted} to the filtration, meaning
\[
r_t \in \mathcal{F}_{t+1} \quad \text{for all } t \in \mathbb{N}_0.
\]
That is, the return $r_t$ is revealed at time $t+1$ (after the decision $f_t$ is made).
\end{assumption}

\section{The Wealth Dynamics}

\begin{definition}[Discrete-Time Wealth Update]
Let $W_t$ denote the agent's wealth at time $t$. The wealth evolves according to
\[
W_t = W_{t-1}(1 + f_t r_t), \quad t \geq 1,
\]
with initial wealth $W_0 > 0$.
\end{definition}

For the Kelly Criterion, we analyze the \textit{log-wealth process}:
\[
\log W_t = \log W_{t-1} + \log(1 + f_t r_t).
\]

Using the Taylor expansion $\log(1 + x) = x - \frac{x^2}{2} + O(x^3)$ for small $|x|$:
\[
\log W_t \approx \log W_{t-1} + f_t r_t - \frac{(f_t r_t)^2}{2}.
\]

Summing over $t = 1, 2, \ldots, T$:
\[
\log W_T \approx \log W_0 + \sum_{t=1}^{T} \left( f_t r_t - \frac{f_t^2 r_t^2}{2} \right).
\]

Taking expectations (assuming $\mathbb{E}[r_t \mid \mathcal{F}_t] = \mu_t$ and $\text{Var}(r_t \mid \mathcal{F}_t) = \sigma_t^2$):
\[
\mathbb{E}[\log W_T \mid \mathcal{F}_0] \approx \log W_0 + \sum_{t=1}^{T} \left( f_t \mu_t - \frac{f_t^2 (\sigma_t^2 + \mu_t^2)}{2} \right).
\]

\section{The Environment}

\subsection{Regime 1: Geometric Brownian Motion (Gaussian Log-Returns)}

Assume returns $r_t$ are i.i.d. Gaussian:
\[
r_t \sim \mathcal{N}(\mu, \sigma^2), \quad t = 1, 2, \ldots
\]

\subsection{Regime 2: Heavy-Tailed Returns (Student-t Distribution)}

To model \textit{fat tails} (extreme events), we use a Student-t distribution:
\[
r_t \sim t_{\nu}(\mu, \sigma^2), \quad \nu = 3,
\]
where $\nu$ is the degrees of freedom. For $\nu = 3$, the distribution has infinite fourth moment, making variance estimation unreliable and inducing rare but severe drawdowns.

\section{The Optimization Problem}

\subsection{Kelly Objective}

\begin{problem}[Kelly Criterion]
Maximize the long-run logarithmic growth rate:
\[
f^* = \arg\max_{f_t} \mathbb{E}\left[ \log(1 + f_t r_t) \mid \mathcal{F}_t \right].
\]
\end{problem}

For i.i.d. Gaussian returns $r_t \sim \mathcal{N}(\mu, \sigma^2)$, the optimal Kelly fraction is:
\[
f^* = \frac{\mu}{\sigma^2}.
\]

\subsection{Risk Constraint}

To prevent catastrophic losses in finite time, we impose two types of risk constraints:

\begin{definition}[Drawdown]
The drawdown at time $t$ is defined as
\[
D_t = \max_{0 \leq s \leq t} W_s - W_t.
\]
The relative drawdown is $D_t / \max_{0 \leq s \leq t} W_s$.
\end{definition}

\begin{problem}[Probability of Drawdown Constraint]
Limit the probability of exceeding a drawdown threshold $\alpha$:
\[
\mathbb{P}(D_t > \alpha) \leq \beta,
\]
where $\alpha$ is the maximum acceptable drawdown (e.g., 50\% of peak wealth) and $\beta$ is the risk tolerance (e.g., 5\%).
\end{problem}

\begin{definition}[Conditional Value at Risk (CVaR)]
For a random variable $X$ (e.g., portfolio return), the $\alpha$-CVaR is
\[
\text{CVaR}_{\alpha}(X) = \mathbb{E}[X \mid X \leq \text{VaR}_{\alpha}(X)],
\]
where $\text{VaR}_{\alpha}(X)$ is the $\alpha$-quantile of $X$.
\end{definition}

\begin{problem}[CVaR-Constrained Kelly Optimization]
For a portfolio of $K$ assets with returns $r_1, \ldots, r_K$, solve:
\begin{align*}
\max_{f_1, \ldots, f_K} \quad & \mathbb{E}\left[ \log\left(1 + \sum_{i=1}^{K} f_i r_i\right) \right] \\
\text{subject to} \quad & \text{CVaR}_{0.05}\left( \sum_{i=1}^{K} f_i r_i \right) \geq -\tau, \\
& \sum_{i=1}^{K} f_i \leq 1, \\
& f_i \geq 0, \quad i = 1, \ldots, K,
\end{align*}
where $\tau$ is the maximum acceptable loss in the worst 5\% of scenarios (e.g., $\tau = 0.20$ for 20\% max loss).
\end{problem}

\section{Connection to Multi-Armed Bandits}

In the multi-armed bandit (MAB) setting, the agent must simultaneously:
\begin{enumerate}
    \item \textbf{Explore}: Learn the unknown means $\mu_1, \ldots, \mu_K$ of $K$ arms.
    \item \textbf{Exploit}: Allocate wealth using Kelly fractions based on current estimates.
\end{enumerate}

Let $\hat{\mu}_{i,t}$ denote the Bayesian posterior mean of arm $i$ at time $t$. Then the Kelly allocation becomes:
\[
f_{i,t} = \frac{\hat{\mu}_{i,t}}{\hat{\sigma}_{i,t}^2}, \quad i = 1, \ldots, K,
\]
subject to the constraints above. The challenge is that incorrect estimates (due to insufficient exploration) lead to suboptimal growth.

\end{document}
